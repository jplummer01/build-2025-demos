## Training an Instruct model into a Reasoning model on AML using GRPO

GRPO (Group Relative Policy Optimization) is a lightweight, memory-efficient reinforcement learning technique designed to train reasoning models by optimising responses without needing a separate value or critique modelâ€”making it faster and more scalable than traditional PPO. It leverages reward functions to guide learning, enabling scalable and stable training of reasoning capabilities.

Video: [https://www.youtube.com/watch?v=YOm_IQt3YWw](https://www.youtube.com/watch?v=YOm_IQt3YWw)

Code: [https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/grpo](https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/grpo)
